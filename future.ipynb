{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70c60be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a881e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_208901/3591124978.py:8: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"Adilbai/stock-dataset\")\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values([\"Ticker\", \"Date\"])\n",
    "\n",
    "# Choose stock\n",
    "ticker = \"AAPL\"\n",
    "stock_df = df[df[\"Ticker\"] == ticker].copy()\n",
    "\n",
    "stock_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74646663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Ticker\"].unique()[:20]\n",
    "sp_df = df[df[\"Ticker\"] == \"^GSPC\"][[\"Date\", \"Close\"]]\n",
    "sp_df = sp_df.rename(columns={\"Close\": \"SP500_Close\"})\n",
    "stock_df = stock_df.merge(\n",
    "    sp_df,\n",
    "    on=\"Date\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3839f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "K_LIST = [120, 60, 40, 20, 10, 5, 1]  # keep within your history\n",
    "HORIZON = 10\n",
    "THRESHOLD = 0.02\n",
    "\n",
    "# 1) daily returns for each stock\n",
    "df = df.sort_values([\"Ticker\", \"Date\"]).copy()\n",
    "df[\"ret_1d\"] = df.groupby(\"Ticker\")[\"Close\"].pct_change()\n",
    "\n",
    "# 2) market proxy return per date (cross-sectional mean)\n",
    "mkt_ret = df.groupby(\"Date\")[\"ret_1d\"].mean().rename(\"mkt_ret_1d\").reset_index()\n",
    "\n",
    "# 3) choose one stock (AAPL here)\n",
    "ticker = \"AAPL\"\n",
    "stock_df = df[df[\"Ticker\"] == ticker].sort_values(\"Date\").copy()\n",
    "\n",
    "# 4) merge market proxy return\n",
    "stock_df = stock_df.merge(mkt_ret, on=\"Date\", how=\"left\")\n",
    "\n",
    "# 5) build synthetic market \"index price\"\n",
    "# Start at 1.0; cumprod of (1 + return)\n",
    "stock_df[\"mkt_price\"] = (1.0 + stock_df[\"mkt_ret_1d\"].fillna(0.0)).cumprod()\n",
    "\n",
    "def k_return(series, k):\n",
    "    return (series / series.shift(k)) - 1.0\n",
    "\n",
    "# 6) compute stock returns, market returns, and relative returns X_k\n",
    "for k in K_LIST:\n",
    "    stock_df[f\"R_stock_{k}\"] = k_return(stock_df[\"Close\"], k)\n",
    "    stock_df[f\"R_mkt_{k}\"]   = k_return(stock_df[\"mkt_price\"], k)\n",
    "    stock_df[f\"X_{k}\"]       = stock_df[f\"R_stock_{k}\"] - stock_df[f\"R_mkt_{k}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2bfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use 1-day relative return X_1 as the “next-day relative return”\n",
    "# and label y(i)=1 if max_{j=1..10} X_1(i+j) >= 2%\n",
    "x1 = stock_df[\"X_1\"].values\n",
    "labels = np.zeros(len(stock_df), dtype=int)\n",
    "\n",
    "for i in range(len(stock_df) - HORIZON):\n",
    "    future_window = x1[i+1:i+1+HORIZON]\n",
    "    labels[i] = int(np.nanmax(future_window) >= THRESHOLD)\n",
    "\n",
    "stock_df[\"label\"] = labels\n",
    "\n",
    "# Trim edges explicitly (enough history for max K and enough future for label)\n",
    "MIN_K = max(K_LIST)\n",
    "stock_df = stock_df.iloc[MIN_K:-HORIZON].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d7b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1115\n",
      "NaNs in features: 0\n",
      "label\n",
      "0    0.556054\n",
      "1    0.443946\n",
      "Name: proportion, dtype: float64\n",
      "      X_120      X_60      X_40      X_20      X_10       X_5       X_1\n",
      "0  0.116847  0.011193 -0.010283  0.066499  0.032245 -0.043683  0.002601\n",
      "1  0.058908 -0.067213 -0.072030 -0.007428 -0.042156 -0.090005 -0.055046\n",
      "2  0.099304 -0.100685  0.015939  0.014755 -0.050056 -0.053824  0.023682\n",
      "3  0.075348 -0.074426  0.034212  0.039751 -0.031558 -0.032186  0.006616\n",
      "4  0.074719 -0.102738 -0.022481  0.002488 -0.059369 -0.046698 -0.022435\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [f\"X_{k}\" for k in K_LIST]\n",
    "print(\"Rows:\", len(stock_df))\n",
    "print(\"NaNs in features:\", stock_df[feature_cols].isna().sum().sum())\n",
    "print(stock_df[\"label\"].value_counts(normalize=True))\n",
    "print(stock_df[feature_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011cf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30aa5c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (892, 7) (223, 7)\n",
      "NaNs after impute: 0 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "X = stock_df[feature_cols].copy()\n",
    "y = stock_df[\"label\"].copy()\n",
    "\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Clean infinities (rare here, but safe)\n",
    "X_train_df = X_train_df.replace([np.inf, -np.inf], np.nan)\n",
    "X_test_df  = X_test_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute (median)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_i = imputer.fit_transform(X_train_df)\n",
    "X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train_i)\n",
    "X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "print(\"Shapes:\", X_train_s.shape, X_test_s.shape)\n",
    "print(\"NaNs after impute:\", np.isnan(X_train_s).sum(), np.isnan(X_test_s).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5331b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "def summarize_metrics(metrics_list):\n",
    "    # metrics_list: list[dict]\n",
    "    keys = metrics_list[0].keys()\n",
    "    summary = {}\n",
    "    for k in keys:\n",
    "        vals = np.array([m[k] for m in metrics_list], dtype=float)\n",
    "        summary[k] = {\"mean\": float(vals.mean()), \"std\": float(vals.std())}\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f456e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows: 40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "TRAIN_DAYS = 253\n",
    "TEST_DAYS = 21\n",
    "GAP = 10\n",
    "\n",
    "def make_sliding_windows(df, train_days=253, gap=10, test_days=21):\n",
    "    windows = []\n",
    "    start = 0\n",
    "    n = len(df)\n",
    "    while start + train_days + gap + test_days <= n:\n",
    "        train = df.iloc[start : start + train_days]\n",
    "        test  = df.iloc[start + train_days + gap : start + train_days + gap + test_days]\n",
    "        windows.append((train, test))\n",
    "        start += test_days  # shift by 1 month (≈21 trading days)\n",
    "    return windows\n",
    "\n",
    "windows = make_sliding_windows(stock_df, TRAIN_DAYS, GAP, TEST_DAYS)\n",
    "print(\"Number of windows:\", len(windows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844ab1c",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54cac821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 0.6098654708520179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", class_weight=\"balanced\")\n",
    "svm.fit(X_train_s, y_train)\n",
    "preds = svm.predict(X_test_s)\n",
    "\n",
    "print(\"SVM accuracy:\", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58436c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM metrics (mean ± std across windows):\n",
      "accuracy: 0.5476 ± 0.2412\n",
      "precision: 0.4395 ± 0.3831\n",
      "recall: 0.4064 ± 0.3686\n",
      "f1: 0.3588 ± 0.3108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm_metrics = []\n",
    "\n",
    "for train, test in windows:\n",
    "    X_train_df = train[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_train = train[\"label\"].astype(int).values\n",
    "\n",
    "    X_test_df = test[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_test = test[\"label\"].astype(int).values\n",
    "\n",
    "    # Impute per-window\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_i = imputer.fit_transform(X_train_df)\n",
    "    X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "    # Scale per-window\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_i)\n",
    "    X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "    svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", class_weight=\"balanced\")\n",
    "    svm.fit(X_train_s, y_train)\n",
    "    preds = svm.predict(X_test_s).astype(int)\n",
    "\n",
    "    svm_metrics.append(compute_metrics(y_test, preds))\n",
    "\n",
    "svm_summary = summarize_metrics(svm_metrics)\n",
    "print(\"SVM metrics (mean ± std across windows):\")\n",
    "for k, v in svm_summary.items():\n",
    "    print(f\"{k}: {v['mean']:.4f} ± {v['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d0aa2",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e849542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF metrics (mean ± std across windows):\n",
      "accuracy: 0.4655 ± 0.2257\n",
      "precision: 0.3405 ± 0.3618\n",
      "recall: 0.3270 ± 0.3578\n",
      "f1: 0.2706 ± 0.2620\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "rf_metrics = []\n",
    "\n",
    "for train, test in windows:\n",
    "    X_train_df = train[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_train = train[\"label\"].astype(int).values\n",
    "\n",
    "    X_test_df = test[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_test = test[\"label\"].astype(int).values\n",
    "\n",
    "    # Impute per-window (no leakage)\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train = imputer.fit_transform(X_train_df)\n",
    "    X_test  = imputer.transform(X_test_df)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,\n",
    "        max_features=min(len(feature_cols), 13),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test).astype(int)\n",
    "\n",
    "    rf_metrics.append(compute_metrics(y_test, preds))\n",
    "\n",
    "rf_summary = summarize_metrics(rf_metrics)\n",
    "print(\"RF metrics (mean ± std across windows):\")\n",
    "for k, v in rf_summary.items():\n",
    "    print(f\"{k}: {v['mean']:.4f} ± {v['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627a511",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1c4f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def make_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        Xs.append(X[i:i+seq_len])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def eval_binary(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"prec\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"rec\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22716e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_window(train_df, test_df, feature_cols):\n",
    "    # keep pandas here; avoid leakage by fitting only on train\n",
    "    X_train_df = train_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test_df  = test_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    y_train = train_df[\"label\"].astype(int).values\n",
    "    y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_i = imputer.fit_transform(X_train_df)\n",
    "    X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_i)\n",
    "    X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "    return X_train_s, y_train, X_test_s, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49bd37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_window(train_df, test_df, feature_cols):\n",
    "    # keep pandas here; avoid leakage by fitting only on train\n",
    "    X_train_df = train_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test_df  = test_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    y_train = train_df[\"label\"].astype(int).values\n",
    "    y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_i = imputer.fit_transform(X_train_df)\n",
    "    X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_i)\n",
    "    X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "    return X_train_s, y_train, X_test_s, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7c9d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 16:06:28.555154: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-08 16:06:28.555393: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-08 16:06:28.585221: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-08 16:06:29.442922: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-08 16:06:29.443285: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_lstm_model(n_features, lstm_units=64, dropout=0.2):\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, return_sequences=True, input_shape=(None, n_features)),\n",
    "        Dropout(dropout),\n",
    "        LSTM(lstm_units),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "760244df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 16:06:43.683848: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f2089bfc0e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f1f82c45da0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM mean acc: 0.65\n",
      "LSTM std acc: 0.4769696007084729\n",
      "LSTM mean f1: 0.25\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 20\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "lstm_metrics = []\n",
    "lstm_accs = []\n",
    "\n",
    "for w_i, (train, test) in enumerate(windows, start=1):\n",
    "    X_train_s, y_train, X_test_s, y_test = preprocess_window(train, test, feature_cols)\n",
    "\n",
    "    X_train_seq, y_train_seq = make_sequences(X_train_s, y_train, SEQ_LEN)\n",
    "    X_test_seq, y_test_seq   = make_sequences(X_test_s, y_test, SEQ_LEN)\n",
    "\n",
    "    # If a window is too small (rare), skip it safely\n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        continue\n",
    "\n",
    "    model = build_lstm_model(n_features=X_train_seq.shape[-1], lstm_units=64, dropout=0.2)\n",
    "\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "    model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "\n",
    "    probs = model.predict(X_test_seq, verbose=0).ravel()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    m = eval_binary(y_test_seq, preds)\n",
    "    lstm_metrics.append(m)\n",
    "    lstm_accs.append(m[\"acc\"])\n",
    "\n",
    "print(\"LSTM mean acc:\", float(np.mean(lstm_accs)))\n",
    "print(\"LSTM std acc:\", float(np.std(lstm_accs)))\n",
    "print(\"LSTM mean f1:\", float(np.mean([m['f1'] for m in lstm_metrics])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b91ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "def summarize(metrics_list):\n",
    "    keys = metrics_list[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        vals = np.array([m[k] for m in metrics_list], dtype=float)\n",
    "        out[k] = (float(vals.mean()), float(vals.std()))\n",
    "    return out\n",
    "\n",
    "def preprocess_window(train_df, test_df, feature_cols):\n",
    "    # pandas -> numpy, fit transforms ONLY on train (no leakage)\n",
    "    X_train_df = train_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test_df  = test_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    y_train = train_df[\"label\"].astype(int).values\n",
    "    y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_i = imputer.fit_transform(X_train_df)\n",
    "    X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_i)\n",
    "    X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "    return X_train_s, y_train, X_test_s, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ed96fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        Xs.append(X[i:i+seq_len])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b29519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "\n",
    "def H_layer(nqubits):\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "def RY_layer(w):\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    for i in range(0, nqubits - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "def q_function(x, q_weights, n_class):\n",
    "    n_dep = q_weights.shape[0]\n",
    "    n_qub = q_weights.shape[1]\n",
    "\n",
    "    H_layer(n_qub)\n",
    "    RY_layer(x)\n",
    "\n",
    "    for k in range(n_dep):\n",
    "        entangling_layer(n_qub)\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_class)]\n",
    "\n",
    "class TorchVQC(nn.Module):\n",
    "    def __init__(self, vqc_depth, n_qubits, n_class):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(0.01 * torch.randn(vqc_depth, n_qubits))\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self.VQC = qml.QNode(q_function, self.dev, interface=\"torch\")\n",
    "        self.n_class = n_class\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: [B, n_qubits]\n",
    "        # returns: [B, n_class]\n",
    "        y_preds = torch.stack([torch.stack(self.VQC(x, self.weights, self.n_class)) for x in X])\n",
    "        return y_preds.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11cfda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vqc_depth):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        n_qubits = input_size + hidden_size\n",
    "\n",
    "        self.input_gate  = TorchVQC(vqc_depth=vqc_depth, n_qubits=n_qubits, n_class=hidden_size)\n",
    "        self.forget_gate = TorchVQC(vqc_depth=vqc_depth, n_qubits=n_qubits, n_class=hidden_size)\n",
    "        self.cell_gate   = TorchVQC(vqc_depth=vqc_depth, n_qubits=n_qubits, n_class=hidden_size)\n",
    "        self.output_gate = TorchVQC(vqc_depth=vqc_depth, n_qubits=n_qubits, n_class=hidden_size)\n",
    "\n",
    "        self.output_post = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        combined = torch.cat((x, h_prev), dim=1)  # [B, input+hidden]\n",
    "\n",
    "        i_t = torch.sigmoid(self.input_gate(combined))\n",
    "        f_t = torch.sigmoid(self.forget_gate(combined))\n",
    "        g_t = torch.tanh(self.cell_gate(combined))\n",
    "        o_t = torch.sigmoid(self.output_gate(combined))\n",
    "\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        out = self.output_post(h_t)  # [B, output_size]\n",
    "        return out, h_t, c_t\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, cell_module):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = cell_module\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [B, T, input_size]\n",
    "        B, T, _ = x.size()\n",
    "        if hidden is None:\n",
    "            h_t = torch.zeros(B, self.hidden_size, device=x.device)\n",
    "            c_t = torch.zeros(B, self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            h_t, c_t = hidden\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            out, h_t, c_t = self.cell(x[:, t, :], (h_t, c_t))\n",
    "            outputs.append(out.unsqueeze(1))  # [B,1,output_size]\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # [B,T,output_size]\n",
    "        return outputs, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c1eecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_qlstm_one_window(\n",
    "    X_train_seq, y_train_seq, X_test_seq, y_test_seq,\n",
    "    hidden_size=4, vqc_depth=1, lr=1e-2, epochs=2,\n",
    "    batch_size=6, seed=0\n",
    "):\n",
    "    # Force CPU to avoid HIP \"invalid device function\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    Xtr = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "    ytr = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "    Xte = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "\n",
    "    input_size = Xtr.shape[-1]\n",
    "    output_size = 1\n",
    "\n",
    "    cell = CustomQLSTMCell(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        vqc_depth=vqc_depth\n",
    "    )\n",
    "\n",
    "    core = CustomLSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        cell_module=cell\n",
    "    )\n",
    "\n",
    "    opt = torch.optim.Adam(core.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    core.train()\n",
    "    n = Xtr.shape[0]\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        perm = torch.randperm(n)  # CPU permutation (no HIP kernel)\n",
    "        for i in range(0, n, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            xb = Xtr[idx]\n",
    "            yb = ytr[idx]\n",
    "\n",
    "            opt.zero_grad()\n",
    "            outputs, _ = core(xb)       # [B,T,1]\n",
    "            logits = outputs[:, -1, 0]  # [B]\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    core.eval()\n",
    "    with torch.no_grad():\n",
    "        out_te, _ = core(Xte)\n",
    "        logits_te = out_te[:, -1, 0]\n",
    "        probs = torch.sigmoid(logits_te).numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff799848",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_qlstm_one_window() got an unexpected keyword argument 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_train_seq) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X_test_seq) == \u001b[32m0\u001b[39m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m preds, probs = \u001b[43mtrain_qlstm_one_window\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvqc_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m m = compute_metrics(y_test_seq, preds)\n\u001b[32m     22\u001b[39m qlstm_metrics.append(m)\n",
      "\u001b[31mTypeError\u001b[39m: train_qlstm_one_window() got an unexpected keyword argument 'device'"
     ]
    }
   ],
   "source": [
    "qlstm_metrics = []\n",
    "\n",
    "for w_i, (train, test) in enumerate(windows[:MAX_WINDOWS], start=1):\n",
    "    X_train_s, y_train, X_test_s, y_test = preprocess_window(train, test, feature_cols)\n",
    "\n",
    "    X_train_seq, y_train_seq = make_sequences(X_train_s, y_train, SEQ_LEN)\n",
    "    X_test_seq, y_test_seq   = make_sequences(X_test_s, y_test, SEQ_LEN)\n",
    "\n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        continue\n",
    "\n",
    "    preds, probs = train_qlstm_one_window(\n",
    "        X_train_seq, y_train_seq, X_test_seq, y_test_seq,\n",
    "        hidden_size=4, vqc_depth=1, lr=1e-2, epochs=4, batch_size=8, seed=42\n",
    "    )\n",
    "\n",
    "    m = compute_metrics(y_test_seq, preds)\n",
    "    qlstm_metrics.append(m)\n",
    "\n",
    "qlstm_summary = summarize(qlstm_metrics)\n",
    "print(\"QLSTM metrics (mean ± std across evaluated windows):\")\n",
    "for k, (mu, sd) in qlstm_summary.items():\n",
    "    print(f\"{k}: {mu:.4f} ± {sd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQCClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=8, vqc_depth=2):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_dim, latent_dim)\n",
    "        self.vqc = TorchVQC(vqc_depth=vqc_depth, n_qubits=latent_dim, n_class=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B,input_dim]\n",
    "        z = torch.tanh(self.fc(x))\n",
    "        logit = self.vqc(z)[:, 0]  # [B]\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ec6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_table(name_to_summary):\n",
    "    # name_to_summary: dict[str, dict(metric -> (mean,std))]\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "    print(\"Model\".ljust(10), *(m.ljust(20) for m in metrics))\n",
    "    for name, summ in name_to_summary.items():\n",
    "        row = [name.ljust(10)]\n",
    "        for m in metrics:\n",
    "            mu, sd = summ[m]\n",
    "            row.append(f\"{mu:.4f} ± {sd:.4f}\".ljust(20))\n",
    "        print(*row)\n",
    "\n",
    "# Example once you have them:\n",
    "# name_to_summary = {\"RF\": rf_summary, \"SVM\": svm_summary, \"LSTM\": lstm_summary, \"QLSTM\": qlstm_summary, ...}\n",
    "# print_summary_table(name_to_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
