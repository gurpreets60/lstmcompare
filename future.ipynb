{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c60be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "\n",
    "MAX_WINDOWS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a881e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "/tmp/ipykernel_248100/3591124978.py:8: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"Adilbai/stock-dataset\")\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "df = df.sort_values([\"Ticker\", \"Date\"])\n",
    "\n",
    "# Choose stock\n",
    "ticker = \"AAPL\"\n",
    "stock_df = df[df[\"Ticker\"] == ticker].copy()\n",
    "\n",
    "stock_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74646663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Ticker\"].unique()[:20]\n",
    "sp_df = df[df[\"Ticker\"] == \"^GSPC\"][[\"Date\", \"Close\"]]\n",
    "sp_df = sp_df.rename(columns={\"Close\": \"SP500_Close\"})\n",
    "stock_df = stock_df.merge(\n",
    "    sp_df,\n",
    "    on=\"Date\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3839f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "K_LIST = [120, 60, 40, 20, 10, 5, 1]  # keep within your history\n",
    "HORIZON = 10\n",
    "THRESHOLD = 0.02\n",
    "\n",
    "# 1) daily returns for each stock\n",
    "df = df.sort_values([\"Ticker\", \"Date\"]).copy()\n",
    "df[\"ret_1d\"] = df.groupby(\"Ticker\")[\"Close\"].pct_change()\n",
    "\n",
    "# 2) market proxy return per date (cross-sectional mean)\n",
    "mkt_ret = df.groupby(\"Date\")[\"ret_1d\"].mean().rename(\"mkt_ret_1d\").reset_index()\n",
    "\n",
    "# 3) choose one stock (AAPL here)\n",
    "ticker = \"AAPL\"\n",
    "stock_df = df[df[\"Ticker\"] == ticker].sort_values(\"Date\").copy()\n",
    "\n",
    "# 4) merge market proxy return\n",
    "stock_df = stock_df.merge(mkt_ret, on=\"Date\", how=\"left\")\n",
    "\n",
    "# 5) build synthetic market \"index price\"\n",
    "# Start at 1.0; cumprod of (1 + return)\n",
    "stock_df[\"mkt_price\"] = (1.0 + stock_df[\"mkt_ret_1d\"].fillna(0.0)).cumprod()\n",
    "\n",
    "def k_return(series, k):\n",
    "    return (series / series.shift(k)) - 1.0\n",
    "\n",
    "# 6) compute stock returns, market returns, and relative returns X_k\n",
    "for k in K_LIST:\n",
    "    stock_df[f\"R_stock_{k}\"] = k_return(stock_df[\"Close\"], k)\n",
    "    stock_df[f\"R_mkt_{k}\"]   = k_return(stock_df[\"mkt_price\"], k)\n",
    "    stock_df[f\"X_{k}\"]       = stock_df[f\"R_stock_{k}\"] - stock_df[f\"R_mkt_{k}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2bfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use 1-day relative return X_1 as the â€œnext-day relative returnâ€\n",
    "# and label y(i)=1 if max_{j=1..10} X_1(i+j) >= 2%\n",
    "x1 = stock_df[\"X_1\"].values\n",
    "labels = np.zeros(len(stock_df), dtype=int)\n",
    "\n",
    "for i in range(len(stock_df) - HORIZON):\n",
    "    future_window = x1[i+1:i+1+HORIZON]\n",
    "    labels[i] = int(np.nanmax(future_window) >= THRESHOLD)\n",
    "\n",
    "stock_df[\"label\"] = labels\n",
    "\n",
    "# Trim edges explicitly (enough history for max K and enough future for label)\n",
    "MIN_K = max(K_LIST)\n",
    "stock_df = stock_df.iloc[MIN_K:-HORIZON].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d7b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1115\n",
      "NaNs in features: 0\n",
      "label\n",
      "0    0.556054\n",
      "1    0.443946\n",
      "Name: proportion, dtype: float64\n",
      "      X_120      X_60      X_40      X_20      X_10       X_5       X_1\n",
      "0  0.116847  0.011193 -0.010283  0.066499  0.032245 -0.043683  0.002601\n",
      "1  0.058908 -0.067213 -0.072030 -0.007428 -0.042156 -0.090005 -0.055046\n",
      "2  0.099304 -0.100685  0.015939  0.014755 -0.050056 -0.053824  0.023682\n",
      "3  0.075348 -0.074426  0.034212  0.039751 -0.031558 -0.032186  0.006616\n",
      "4  0.074719 -0.102738 -0.022481  0.002488 -0.059369 -0.046698 -0.022435\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [f\"X_{k}\" for k in K_LIST]\n",
    "print(\"Rows:\", len(stock_df))\n",
    "print(\"NaNs in features:\", stock_df[feature_cols].isna().sum().sum())\n",
    "print(stock_df[\"label\"].value_counts(normalize=True))\n",
    "print(stock_df[feature_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a011cf3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30aa5c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (892, 7) (223, 7)\n",
      "NaNs after impute: 0 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "X = stock_df[feature_cols].copy()\n",
    "y = stock_df[\"label\"].copy()\n",
    "\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Clean infinities (rare here, but safe)\n",
    "X_train_df = X_train_df.replace([np.inf, -np.inf], np.nan)\n",
    "X_test_df  = X_test_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute (median)\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_i = imputer.fit_transform(X_train_df)\n",
    "X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train_i)\n",
    "X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "print(\"Shapes:\", X_train_s.shape, X_test_s.shape)\n",
    "print(\"NaNs after impute:\", np.isnan(X_train_s).sum(), np.isnan(X_test_s).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5331b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "def summarize_metrics(metrics_list):\n",
    "    # metrics_list: list[dict]\n",
    "    keys = metrics_list[0].keys()\n",
    "    summary = {}\n",
    "    for k in keys:\n",
    "        vals = np.array([m[k] for m in metrics_list], dtype=float)\n",
    "        summary[k] = {\"mean\": float(vals.mean()), \"std\": float(vals.std())}\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6b27a8",
   "metadata": {},
   "source": [
    "Important, area where we can define the number of windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f456e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of windows: 40\n",
      "Windows used for evaluation: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "TRAIN_DAYS = 253\n",
    "TEST_DAYS = 21\n",
    "GAP = 10\n",
    "\n",
    "def make_sliding_windows(df, train_days=253, gap=10, test_days=21):\n",
    "    windows = []\n",
    "    start = 0\n",
    "    n = len(df)\n",
    "    while start + train_days + gap + test_days <= n:\n",
    "        train = df.iloc[start : start + train_days]\n",
    "        test  = df.iloc[start + train_days + gap : start + train_days + gap + test_days]\n",
    "        windows.append((train, test))\n",
    "        start += test_days  # shift by 1 month (â‰ˆ21 trading days)\n",
    "    return windows\n",
    "\n",
    "windows = make_sliding_windows(stock_df, TRAIN_DAYS, GAP, TEST_DAYS)\n",
    "print(\"Number of windows:\", len(windows))\n",
    "\n",
    "MAX_WINDOWS = 3   # ðŸ” change to 40 later if you want\n",
    "windows = windows[:MAX_WINDOWS]\n",
    "\n",
    "print(\"Windows used for evaluation:\", len(windows))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1844ab1c",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54cac821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy: 0.6098654708520179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", class_weight=\"balanced\")\n",
    "svm.fit(X_train_s, y_train)\n",
    "preds = svm.predict(X_test_s)\n",
    "\n",
    "print(\"SVM accuracy:\", accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58436c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM metrics (mean Â± std across windows):\n",
      "accuracy: 0.4603 Â± 0.0594\n",
      "precision: 0.3436 Â± 0.3362\n",
      "recall: 0.2619 Â± 0.2048\n",
      "f1: 0.2456 Â± 0.1789\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm_metrics = []\n",
    "\n",
    "for train, test in windows:\n",
    "    X_train_df = train[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_train = train[\"label\"].astype(int).values\n",
    "\n",
    "    X_test_df = test[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_test = test[\"label\"].astype(int).values\n",
    "\n",
    "    # Impute per-window\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_i = imputer.fit_transform(X_train_df)\n",
    "    X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "    # Scale per-window\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_i)\n",
    "    X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "    svm = SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", class_weight=\"balanced\")\n",
    "    svm.fit(X_train_s, y_train)\n",
    "    preds = svm.predict(X_test_s).astype(int)\n",
    "\n",
    "    svm_metrics.append(compute_metrics(y_test, preds))\n",
    "\n",
    "svm_summary = summarize_metrics(svm_metrics)\n",
    "print(\"SVM metrics (mean Â± std across windows):\")\n",
    "for k, v in svm_summary.items():\n",
    "    print(f\"{k}: {v['mean']:.4f} Â± {v['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d0aa2",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e849542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF metrics (mean Â± std across windows):\n",
      "accuracy: 0.5238 Â± 0.1166\n",
      "precision: 0.7556 Â± 0.3457\n",
      "recall: 0.3915 Â± 0.1963\n",
      "f1: 0.3963 Â± 0.0347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "rf_metrics = []\n",
    "\n",
    "for train, test in windows:\n",
    "    X_train_df = train[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_train = train[\"label\"].astype(int).values\n",
    "\n",
    "    X_test_df = test[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_test = test[\"label\"].astype(int).values\n",
    "\n",
    "    # Impute per-window (no leakage)\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train = imputer.fit_transform(X_train_df)\n",
    "    X_test  = imputer.transform(X_test_df)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,\n",
    "        max_features=min(len(feature_cols), 13),\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "    preds = rf.predict(X_test).astype(int)\n",
    "\n",
    "    rf_metrics.append(compute_metrics(y_test, preds))\n",
    "\n",
    "rf_summary = summarize_metrics(rf_metrics)\n",
    "print(\"RF metrics (mean Â± std across windows):\")\n",
    "for k, v in rf_summary.items():\n",
    "    print(f\"{k}: {v['mean']:.4f} Â± {v['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627a511",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c4f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def make_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        Xs.append(X[i:i+seq_len])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "def eval_binary(y_true, y_pred):\n",
    "    return {\n",
    "        \"acc\": accuracy_score(y_true, y_pred),\n",
    "        \"prec\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"rec\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22716e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_window(train_df, test_df, feature_cols):\n",
    "    # keep pandas here; avoid leakage by fitting only on train\n",
    "    X_train_df = train_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test_df  = test_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    y_train = train_df[\"label\"].astype(int).values\n",
    "    y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_i = imputer.fit_transform(X_train_df)\n",
    "    X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_i)\n",
    "    X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "    return X_train_s, y_train, X_test_s, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49bd37f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_window(train_df, test_df, feature_cols):\n",
    "    # keep pandas here; avoid leakage by fitting only on train\n",
    "    X_train_df = train_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test_df  = test_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    y_train = train_df[\"label\"].astype(int).values\n",
    "    y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_i = imputer.fit_transform(X_train_df)\n",
    "    X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_i)\n",
    "    X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "    return X_train_s, y_train, X_test_s, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7c9d0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 19:01:47.252492: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2026-02-08 19:01:47.252718: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-08 19:01:47.277168: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-08 19:01:48.098310: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-08 19:01:48.098670: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_lstm_model(n_features, lstm_units=64, dropout=0.2):\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, return_sequences=True, input_shape=(None, n_features)),\n",
    "        Dropout(dropout),\n",
    "        LSTM(lstm_units),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "760244df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-08 19:01:59.141329: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "/home/gurpreet/.conda/envs/lstm/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM mean acc: 0.6666666666666666\n",
      "LSTM std acc: 0.4714045207910317\n",
      "LSTM mean f1: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "SEQ_LEN = 20\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "lstm_metrics = []\n",
    "lstm_accs = []\n",
    "\n",
    "for w_i, (train, test) in enumerate(windows, start=1):\n",
    "    X_train_s, y_train, X_test_s, y_test = preprocess_window(train, test, feature_cols)\n",
    "\n",
    "    X_train_seq, y_train_seq = make_sequences(X_train_s, y_train, SEQ_LEN)\n",
    "    X_test_seq, y_test_seq   = make_sequences(X_test_s, y_test, SEQ_LEN)\n",
    "\n",
    "    # If a window is too small (rare), skip it safely\n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        continue\n",
    "\n",
    "    model = build_lstm_model(n_features=X_train_seq.shape[-1], lstm_units=64, dropout=0.2)\n",
    "\n",
    "    es = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "    model.fit(\n",
    "        X_train_seq, y_train_seq,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=0,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "\n",
    "    probs = model.predict(X_test_seq, verbose=0).ravel()\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    m = eval_binary(y_test_seq, preds)\n",
    "    lstm_metrics.append(m)\n",
    "    lstm_accs.append(m[\"acc\"])\n",
    "\n",
    "print(\"LSTM mean acc:\", float(np.mean(lstm_accs)))\n",
    "print(\"LSTM std acc:\", float(np.std(lstm_accs)))\n",
    "print(\"LSTM mean f1:\", float(np.mean([m['f1'] for m in lstm_metrics])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edd2fcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_summary (mean Â± std across windows):\n",
      "acc: 0.6667 Â± 0.4714\n",
      "prec: 0.3333 Â± 0.4714\n",
      "rec: 0.3333 Â± 0.4714\n",
      "f1: 0.3333 Â± 0.4714\n"
     ]
    }
   ],
   "source": [
    "lstm_summary = {}\n",
    "for k in [\"acc\", \"prec\", \"rec\", \"f1\"]:\n",
    "    vals = np.array([m[k] for m in lstm_metrics], dtype=float)\n",
    "    lstm_summary[k] = (float(vals.mean()), float(vals.std()))\n",
    "\n",
    "print(\"lstm_summary (mean Â± std across windows):\")\n",
    "for k, (mu, sd) in lstm_summary.items():\n",
    "    print(f\"{k}: {mu:.4f} Â± {sd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f23384",
   "metadata": {},
   "source": [
    "QLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37b91ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "def summarize(metrics_list):\n",
    "    keys = metrics_list[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        vals = np.array([m[k] for m in metrics_list], dtype=float)\n",
    "        out[k] = (float(vals.mean()), float(vals.std()))\n",
    "    return out\n",
    "\n",
    "def preprocess_window(train_df, test_df, feature_cols):\n",
    "    # pandas -> numpy, fit transforms ONLY on train (no leakage)\n",
    "    X_train_df = train_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    X_test_df  = test_df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    y_train = train_df[\"label\"].astype(int).values\n",
    "    y_test  = test_df[\"label\"].astype(int).values\n",
    "\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    X_train_i = imputer.fit_transform(X_train_df)\n",
    "    X_test_i  = imputer.transform(X_test_df)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train_i)\n",
    "    X_test_s  = scaler.transform(X_test_i)\n",
    "\n",
    "    return X_train_s, y_train, X_test_s, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed96fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        Xs.append(X[i:i+seq_len])\n",
    "        ys.append(y[i+seq_len])\n",
    "    return np.array(Xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b29519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pennylane as qml\n",
    "\n",
    "def H_layer(nqubits):\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "def RY_layer(w):\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    for i in range(0, nqubits - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "def q_function(x, q_weights, n_class):\n",
    "    n_dep = q_weights.shape[0]\n",
    "    n_qub = q_weights.shape[1]\n",
    "\n",
    "    H_layer(n_qub)\n",
    "    RY_layer(x)\n",
    "\n",
    "    for k in range(n_dep):\n",
    "        entangling_layer(n_qub)\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_class)]\n",
    "\n",
    "class TorchVQC(nn.Module):\n",
    "    def __init__(self, vqc_depth, n_qubits, n_class):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(0.01 * torch.randn(vqc_depth, n_qubits))\n",
    "        self.dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "        self.VQC = qml.QNode(q_function, self.dev, interface=\"torch\")\n",
    "        self.n_class = n_class\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: [B, n_qubits]\n",
    "        # returns: [B, n_class]\n",
    "        y_preds = torch.stack([torch.stack(self.VQC(x, self.weights, self.n_class)) for x in X])\n",
    "        return y_preds.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11cfda9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vqc_depth):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        n_qubits = input_size + hidden_size\n",
    "\n",
    "        self.input_gate  = TorchVQC(vqc_depth=vqc_depth, n_qubits=n_qubits, n_class=hidden_size)\n",
    "        self.forget_gate = TorchVQC(vqc_depth=vqc_depth, n_qubits=n_qubits, n_class=hidden_size)\n",
    "        self.cell_gate   = TorchVQC(vqc_depth=vqc_depth, n_qubits=n_qubits, n_class=hidden_size)\n",
    "        self.output_gate = TorchVQC(vqc_depth=vqc_depth, n_qubits=n_qubits, n_class=hidden_size)\n",
    "\n",
    "        self.output_post = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        combined = torch.cat((x, h_prev), dim=1)  # [B, input+hidden]\n",
    "\n",
    "        i_t = torch.sigmoid(self.input_gate(combined))\n",
    "        f_t = torch.sigmoid(self.forget_gate(combined))\n",
    "        g_t = torch.tanh(self.cell_gate(combined))\n",
    "        o_t = torch.sigmoid(self.output_gate(combined))\n",
    "\n",
    "        c_t = f_t * c_prev + i_t * g_t\n",
    "        h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "        out = self.output_post(h_t)  # [B, output_size]\n",
    "        return out, h_t, c_t\n",
    "\n",
    "class CustomLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, cell_module):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = cell_module\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [B, T, input_size]\n",
    "        B, T, _ = x.size()\n",
    "        if hidden is None:\n",
    "            h_t = torch.zeros(B, self.hidden_size, device=x.device)\n",
    "            c_t = torch.zeros(B, self.hidden_size, device=x.device)\n",
    "        else:\n",
    "            h_t, c_t = hidden\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(T):\n",
    "            out, h_t, c_t = self.cell(x[:, t, :], (h_t, c_t))\n",
    "            outputs.append(out.unsqueeze(1))  # [B,1,output_size]\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # [B,T,output_size]\n",
    "        return outputs, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c1eecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def train_qlstm_one_window(\n",
    "    X_train_seq, y_train_seq, X_test_seq, y_test_seq,\n",
    "    hidden_size=4, vqc_depth=1, lr=1e-2, epochs=2,\n",
    "    batch_size=6, seed=0\n",
    "):\n",
    "    # Force CPU to avoid HIP \"invalid device function\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    Xtr = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "    ytr = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "    Xte = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "\n",
    "    input_size = Xtr.shape[-1]\n",
    "    output_size = 1\n",
    "\n",
    "    cell = CustomQLSTMCell(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=output_size,\n",
    "        vqc_depth=vqc_depth\n",
    "    )\n",
    "\n",
    "    core = CustomLSTM(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        cell_module=cell\n",
    "    )\n",
    "\n",
    "    opt = torch.optim.Adam(core.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    core.train()\n",
    "    n = Xtr.shape[0]\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        perm = torch.randperm(n)  # CPU permutation (no HIP kernel)\n",
    "        for i in range(0, n, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            xb = Xtr[idx]\n",
    "            yb = ytr[idx]\n",
    "\n",
    "            opt.zero_grad()\n",
    "            outputs, _ = core(xb)       # [B,T,1]\n",
    "            logits = outputs[:, -1, 0]  # [B]\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    core.eval()\n",
    "    with torch.no_grad():\n",
    "        out_te, _ = core(Xte)\n",
    "        logits_te = out_te[:, -1, 0]\n",
    "        probs = torch.sigmoid(logits_te).numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    return preds, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6a66d",
   "metadata": {},
   "source": [
    "This QLSTM cannot accurately be compared with htun's lstm, but we can compare their implementations.\n",
    "\n",
    "Htun et al. used  ~494 stocks and 23 windows\n",
    "\n",
    "We are using 1 stock and 3 windows for testing throughout in the first runthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff799848",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlstm_metrics = []\n",
    "SEQ_LEN = 10\n",
    "\n",
    "for w_i, (train, test) in enumerate(windows[:MAX_WINDOWS], start=1):\n",
    "    X_train_s, y_train, X_test_s, y_test = preprocess_window(train, test, feature_cols)\n",
    "\n",
    "    X_train_seq, y_train_seq = make_sequences(X_train_s, y_train, SEQ_LEN)\n",
    "    X_test_seq, y_test_seq   = make_sequences(X_test_s, y_test, SEQ_LEN)\n",
    "\n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        continue\n",
    "\n",
    "    preds, probs = train_qlstm_one_window(\n",
    "        X_train_seq, y_train_seq, X_test_seq, y_test_seq,\n",
    "        hidden_size=4, vqc_depth=1, lr=1e-2, epochs=4, batch_size=8, seed=42\n",
    "    )\n",
    "\n",
    "    m = compute_metrics(y_test_seq, preds)\n",
    "    qlstm_metrics.append(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "676b04fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " [{'accuracy': 0.2727272727272727, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0},\n",
       "  {'accuracy': 0.8181818181818182,\n",
       "   'precision': 0.8181818181818182,\n",
       "   'recall': 1.0,\n",
       "   'f1': 0.9}])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qlstm_metrics), qlstm_metrics[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "957b9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsummarize(metrics_list):\n",
    "    keys = metrics_list[0].keys()\n",
    "    out = {}\n",
    "    for k in keys:\n",
    "        vals = np.array([m[k] for m in metrics_list], dtype=float)\n",
    "        out[k] = (float(vals.mean()), float(vals.std()))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8f3eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLSTM metrics (mean Â± std across evaluated windows):\n",
      "accuracy: 0.6364 Â± 0.2571\n",
      "precision: 0.6061 Â± 0.4349\n",
      "recall: 0.5333 Â± 0.4110\n",
      "f1: 0.5500 Â± 0.3937\n"
     ]
    }
   ],
   "source": [
    "qlstm_summary = newsummarize(qlstm_metrics)\n",
    "print(\"QLSTM metrics (mean Â± std across evaluated windows):\")\n",
    "for k, (mu, sd) in qlstm_summary.items():\n",
    "    print(f\"{k}: {mu:.4f} Â± {sd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b826403",
   "metadata": {},
   "source": [
    "TORCH VQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d95e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchVQC(nn.Module):\n",
    "\tdef __init__(self, vqc_depth, n_qubits, n_class):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.weights = nn.Parameter(0.01 * torch.randn(vqc_depth, n_qubits))  # g rotation params\n",
    "\t\tself.dev = qml.device(\"default.qubit\", wires=n_qubits)  # Can use different simulation backend or quantum computers.\n",
    "\t\tself.VQC = qml.QNode(q_function, self.dev, interface = \"torch\")\n",
    "\n",
    "\t\tself.n_class = n_class\n",
    "\n",
    "\n",
    "\tdef forward(self, X):\n",
    "\t\ty_preds = torch.stack([torch.stack(self.VQC(x, self.weights, self.n_class)).float() for x in X]) # PennyLane 0.35.1\n",
    "\t\treturn y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8de04d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQCFeedForwardClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward classifier: X (batch, F) -> binary logit.\n",
    "    Projects features to latent_dim, runs TorchVQC, then outputs 1 logit.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, latent_dim=8, vqc_depth=2):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.pre = nn.Linear(in_dim, latent_dim)\n",
    "        self.vqc = TorchVQC(vqc_depth=vqc_depth, n_qubits=latent_dim, n_class=latent_dim)\n",
    "        self.post = nn.Linear(latent_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, F)\n",
    "        z = torch.tanh(self.pre(x))\n",
    "        z = torch.tanh(self.vqc(z))   # (B, latent_dim)\n",
    "        logit = self.post(z).squeeze(-1)  # (B,)\n",
    "        return logit\n",
    "\n",
    "\n",
    "def train_vqc_ff_one_window(X_train_s, y_train, X_test_s, y_test,\n",
    "                            epochs=2, batch_size=32, lr=1e-2,\n",
    "                            latent_dim=8, vqc_depth=2, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    device = \"cpu\"  # keep CPU stable for PennyLane\n",
    "    Xtr = torch.tensor(X_train_s, dtype=torch.float32, device=device)\n",
    "    ytr = torch.tensor(y_train.astype(np.float32), dtype=torch.float32, device=device)\n",
    "    Xte = torch.tensor(X_test_s, dtype=torch.float32, device=device)\n",
    "\n",
    "    model = VQCFeedForwardClassifier(in_dim=Xtr.shape[1], latent_dim=latent_dim, vqc_depth=vqc_depth).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    n = Xtr.shape[0]\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        perm = torch.randperm(n)  # CPU-safe\n",
    "        losses = []\n",
    "\n",
    "        for i in range(0, n, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            xb = Xtr[idx]\n",
    "            yb = ytr[idx]\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        print(f\"[TorchVQC-FF] epoch {ep}/{epochs} train_loss={np.mean(losses):.6f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(Xte).cpu().numpy()\n",
    "        probs = 1 / (1 + np.exp(-logits))\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    return preds, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "278c75f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TorchVQC-FF] epoch 1/2 train_loss=0.690107\n",
      "[TorchVQC-FF] epoch 2/2 train_loss=0.671609\n",
      "[TorchVQC-FF] epoch 1/2 train_loss=0.686205\n",
      "[TorchVQC-FF] epoch 2/2 train_loss=0.667958\n",
      "[TorchVQC-FF] epoch 1/2 train_loss=0.679398\n",
      "[TorchVQC-FF] epoch 2/2 train_loss=0.664912\n",
      "TorchVQC-FF summary (mean Â± std across windows):\n",
      "acc: 0.3810 Â± 0.1029\n",
      "prec: 0.3929 Â± 0.2592\n",
      "rec: 0.5132 Â± 0.3678\n",
      "f1: 0.3716 Â± 0.1792\n"
     ]
    }
   ],
   "source": [
    "VQC_EPOCHS = 2\n",
    "VQC_BATCH  = 32\n",
    "\n",
    "vqc_ff_metrics = []\n",
    "vqc_ff_accs = []\n",
    "\n",
    "for w_i, (train, test) in enumerate(windows, start=1):\n",
    "    X_train_s, y_train, X_test_s, y_test = preprocess_window(train, test, feature_cols)\n",
    "\n",
    "    preds, probs = train_vqc_ff_one_window(\n",
    "        X_train_s, y_train, X_test_s, y_test,\n",
    "        epochs=VQC_EPOCHS, batch_size=VQC_BATCH, lr=1e-2,\n",
    "        latent_dim=8, vqc_depth=2, seed=42\n",
    "    )\n",
    "\n",
    "    m = eval_binary(y_test, preds)\n",
    "    vqc_ff_metrics.append(m)\n",
    "    vqc_ff_accs.append(m[\"acc\"])\n",
    "\n",
    "vqc_ff_summary = {k: (float(np.mean([m[k] for m in vqc_ff_metrics])),\n",
    "                      float(np.std([m[k] for m in vqc_ff_metrics])))\n",
    "                  for k in [\"acc\",\"prec\",\"rec\",\"f1\"]}\n",
    "\n",
    "print(\"TorchVQC-FF summary (mean Â± std across windows):\")\n",
    "for k, (mu, sd) in vqc_ff_summary.items():\n",
    "    print(f\"{k}: {mu:.4f} Â± {sd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1699db",
   "metadata": {},
   "source": [
    "FWP VQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "928a22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### VQC\n",
    "\n",
    "def H_layer(nqubits):\n",
    "\t\"\"\"Layer of single-qubit Hadamard gates.\n",
    "\t\"\"\"\n",
    "\tfor idx in range(nqubits):\n",
    "\t\tqml.Hadamard(wires=idx)\n",
    "\n",
    "def RX_layer(w):\n",
    "\t\"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "\t\"\"\"\n",
    "\tfor idx, element in enumerate(w):\n",
    "\t\tqml.RX(element, wires=idx)\n",
    "\n",
    "def RY_layer(w):\n",
    "\t\"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "\t\"\"\"\n",
    "\tfor idx, element in enumerate(w):\n",
    "\t\tqml.RY(element, wires=idx)\n",
    "\n",
    "def RZ_layer(w):\n",
    "\t\"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "\t\"\"\"\n",
    "\tfor idx, element in enumerate(w):\n",
    "\t\tqml.RZ(element, wires=idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "\t\"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "\t\"\"\"\n",
    "\t# In other words it should apply something like :\n",
    "\t# CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "\t#   CNOT  CNOT  CNOT...  CNOT\n",
    "\tfor i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
    "\t\tqml.CNOT(wires=[i, i + 1])\n",
    "\tfor i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
    "\t\tqml.CNOT(wires=[i, i + 1])\n",
    "\t\n",
    "\n",
    "def cycle_entangling_layer(nqubits):\n",
    "\tfor i in range(0, nqubits):  \n",
    "\t\tqml.CNOT(wires=[i, (i + 1) % nqubits])\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "### VQC function\n",
    "\n",
    "def quantum_net(inputs, q_weights, n_outputs):\n",
    "\tn_dep = q_weights.shape[0]\n",
    "\tn_qub = q_weights.shape[1]\n",
    "\n",
    "\tH_layer(n_qub)\n",
    "\n",
    "\tRY_layer(inputs)\n",
    "\n",
    "\tfor k in range(n_dep):\n",
    "\t\tentangling_layer(n_qub)\n",
    "\t\tRY_layer(q_weights[k])\n",
    "\n",
    "\n",
    "\treturn [qml.expval(qml.PauliZ(position)) for position in range(n_outputs)]\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "class HideSignature:\n",
    "\tdef __init__(self, partial_func):\n",
    "\t\tself.partial_func = partial_func\n",
    "\n",
    "\tdef __call__(self, inputs, q_weights):\n",
    "\t\treturn self.partial_func(inputs, q_weights)\n",
    "\n",
    "##############\n",
    "\n",
    "### VQC batch wrapper\n",
    "\n",
    "class BatchVQC:\n",
    "\tdef __init__(self, q_func):\n",
    "\t\tself.q_func = q_func\n",
    "\n",
    "\tdef __call__(self, inputs, q_weights):\n",
    "\n",
    "\t\tres_all = []\n",
    "\t\tfor input_item, q_weight_item in zip(inputs, q_weights):\n",
    "\t\t\tres = self.q_func(input_item, q_weight_item) \n",
    "\t\t\tres_all.append(torch.stack(res)) \n",
    "\n",
    "\t\treturn torch.stack(res_all)\n",
    "\n",
    "\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f656dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# --- reuse your H_layer, RY_layer, entangling_layer, quantum_net, HideSignature, BatchVQC exactly ---\n",
    "# (Make sure they are defined)\n",
    "\n",
    "class FWPCellCls(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim=4, latent_dim=8, q_depth=2):\n",
    "        super().__init__()\n",
    "        self.n_qubits = latent_dim\n",
    "        self.q_depth = q_depth\n",
    "\n",
    "        dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "        self.q_func = BatchVQC(qml.QNode(HideSignature(partial(quantum_net, n_outputs=a_dim)), dev, interface=\"torch\"))\n",
    "\n",
    "        self.slow_program_encoder = nn.Linear(s_dim, latent_dim)\n",
    "        self.slow_program_layer_idx = nn.Linear(latent_dim, self.q_depth)\n",
    "        self.slow_program_qubit_idx = nn.Linear(latent_dim, self.n_qubits)\n",
    "\n",
    "        # Output a single LOGIT\n",
    "        self.post_processing = nn.Linear(a_dim, 1)\n",
    "\n",
    "    def forward(self, batch_item, previous_circuit_param):\n",
    "        # batch_item: (B, s_dim)\n",
    "        res = self.slow_program_encoder(batch_item)\n",
    "        res_layer_idx = self.slow_program_layer_idx(res)\n",
    "        res_qubit_idx = self.slow_program_qubit_idx(res)\n",
    "\n",
    "        out_circuit_params = []\n",
    "        for layer_idx, qubit_idx in zip(res_layer_idx, res_qubit_idx):\n",
    "            out_circuit_params.append(torch.outer(layer_idx, qubit_idx))\n",
    "        out_circuit_params = torch.stack(out_circuit_params)\n",
    "\n",
    "        out_circuit_params = out_circuit_params + previous_circuit_param\n",
    "\n",
    "        # quantum output: (B, a_dim)\n",
    "        q_out = self.q_func(batch_item, out_circuit_params)\n",
    "\n",
    "        logit = self.post_processing(q_out).squeeze(-1)  # (B,)\n",
    "        return logit, out_circuit_params\n",
    "\n",
    "    def initial_fast_params(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.q_depth, self.n_qubits)\n",
    "\n",
    "\n",
    "class FWPCls(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim=4, latent_dim=8, q_depth=2):\n",
    "        super().__init__()\n",
    "        self.cell = FWPCellCls(s_dim=s_dim, a_dim=a_dim, latent_dim=latent_dim, q_depth=q_depth)\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        # x_seq: (B, T, s_dim)\n",
    "        B, T, _ = x_seq.shape\n",
    "        fast = self.cell.initial_fast_params(B)\n",
    "\n",
    "        logits_over_time = []\n",
    "        for t in range(T):\n",
    "            logit_t, fast = self.cell(x_seq[:, t, :], fast)\n",
    "            logits_over_time.append(logit_t)\n",
    "\n",
    "        # (T, B)\n",
    "        return torch.stack(logits_over_time, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa319be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# ---------- VQC building blocks ----------\n",
    "def H_layer(nqubits):\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "def RY_layer(w):\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    for i in range(0, nqubits - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "def quantum_net(inputs, q_weights, n_outputs):\n",
    "    n_dep = q_weights.shape[0]\n",
    "    n_qub = q_weights.shape[1]\n",
    "\n",
    "    H_layer(n_qub)\n",
    "    RY_layer(inputs)\n",
    "\n",
    "    for k in range(n_dep):\n",
    "        entangling_layer(n_qub)\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_outputs)]\n",
    "\n",
    "class HideSignature:\n",
    "    def __init__(self, partial_func):\n",
    "        self.partial_func = partial_func\n",
    "    def __call__(self, inputs, q_weights):\n",
    "        return self.partial_func(inputs, q_weights)\n",
    "\n",
    "class BatchVQC:\n",
    "    \"\"\"\n",
    "    inputs:   (B, n_qubits)\n",
    "    q_weights:(B, q_depth, n_qubits)\n",
    "    returns:  (B, a_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, qnode):\n",
    "        self.qnode = qnode\n",
    "\n",
    "    def __call__(self, inputs, q_weights):\n",
    "        outs = []\n",
    "        for x_i, w_i in zip(inputs, q_weights):\n",
    "            res = self.qnode(x_i, w_i)           # list length a_dim\n",
    "            outs.append(torch.stack(res))        # (a_dim,)\n",
    "        return torch.stack(outs)                 # (B, a_dim)\n",
    "\n",
    "# ---------- FWP ----------\n",
    "class FWPCell(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim=4, latent_dim=8, q_depth=2):\n",
    "        super().__init__()\n",
    "        self.n_qubits = latent_dim\n",
    "        self.q_depth = q_depth\n",
    "        self.a_dim = a_dim\n",
    "\n",
    "        dev = qml.device(\"default.qubit\", wires=self.n_qubits)\n",
    "\n",
    "        qnode = qml.QNode(\n",
    "            HideSignature(partial(quantum_net, n_outputs=a_dim)),\n",
    "            dev,\n",
    "            interface=\"torch\"\n",
    "        )\n",
    "        self.q_func = BatchVQC(qnode)\n",
    "\n",
    "        # Encode s_dim -> latent_dim (n_qubits)\n",
    "        self.slow_program_encoder = nn.Linear(s_dim, latent_dim)\n",
    "        self.slow_program_layer_idx = nn.Linear(latent_dim, q_depth)\n",
    "        self.slow_program_qubit_idx = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        # Post-process a_dim -> 1 logit\n",
    "        self.post_processing = nn.Linear(a_dim, 1)\n",
    "\n",
    "    def initial_fast_params(self, batch_size, device):\n",
    "        return torch.zeros(batch_size, self.q_depth, self.n_qubits, device=device, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, batch_item, prev_params):\n",
    "        \"\"\"\n",
    "        batch_item: (B, s_dim)\n",
    "        prev_params:(B, q_depth, n_qubits)\n",
    "        \"\"\"\n",
    "        z = self.slow_program_encoder(batch_item)            # (B, latent_dim)\n",
    "        layer_idx = self.slow_program_layer_idx(z)           # (B, q_depth)\n",
    "        qubit_idx = self.slow_program_qubit_idx(z)           # (B, n_qubits)\n",
    "\n",
    "        # Outer product per batch: (q_depth, n_qubits)\n",
    "        out_params = torch.stack([torch.outer(l, q) for l, q in zip(layer_idx, qubit_idx)], dim=0)\n",
    "        out_params = out_params + prev_params                # (B, q_depth, n_qubits)\n",
    "        out_params = out_params.float()\n",
    "\n",
    "        # Quantum eval expects inputs shaped (B, n_qubits)\n",
    "        # So ensure we feed encoded features (z) into circuit inputs\n",
    "        \n",
    "        # If prefer raw batch_item, need s_dim == n_qubits.\n",
    "        vqc_in = z                                            # (B, n_qubits)\n",
    "        vqc_out = self.q_func(vqc_in, out_params).float()  # <-- FORCE float32\n",
    "        logit = self.post_processing(vqc_out)              # now float32 x float32\n",
    "\n",
    "        #logit = self.post_processing(vqc_out)                  # (B, 1)\n",
    "        return logit, out_params\n",
    "\n",
    "class FWP(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence model. Input: (B, T, s_dim). Output: (T, B, 1) logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, s_dim, a_dim=4, latent_dim=8, q_depth=2):\n",
    "        super().__init__()\n",
    "        self.cell = FWPCell(s_dim=s_dim, a_dim=a_dim, latent_dim=latent_dim, q_depth=q_depth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, s_dim)\n",
    "        B, T, _ = x.shape\n",
    "        device = x.device\n",
    "        params = self.cell.initial_fast_params(B, device=device)\n",
    "        \n",
    "\n",
    "        outs = []\n",
    "        for t in range(T):\n",
    "            logit, params = self.cell(x[:, t, :], params)  # logit: (B,1)\n",
    "            outs.append(logit)\n",
    "        return torch.stack(outs, dim=0)  # (T, B, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c2ff716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fwp_one_window(X_train_seq, y_train_seq, X_test_seq, y_test_seq,\n",
    "                         epochs=2, batch_size=16, lr=1e-2,\n",
    "                         a_dim=4, latent_dim=8, q_depth=2, seed=42,\n",
    "                         device=\"cpu\"):\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---- FORCE FLOAT32 ----\n",
    "    Xtr = torch.tensor(X_train_seq, dtype=torch.float32, device=device)\n",
    "    ytr = torch.tensor(y_train_seq, dtype=torch.float32, device=device)\n",
    "    Xte = torch.tensor(X_test_seq,  dtype=torch.float32, device=device)\n",
    "    yte = torch.tensor(y_test_seq,  dtype=torch.float32, device=device)\n",
    "\n",
    "    # Build model (must match dtype)\n",
    "    model = FWP(s_dim=Xtr.shape[-1], a_dim=a_dim, latent_dim=latent_dim, q_depth=q_depth).to(device).float()\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    n = Xtr.shape[0]\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        perm = torch.randperm(n, device=device)\n",
    "        ep_loss = 0.0\n",
    "\n",
    "        for i in range(0, n, batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            xb = Xtr[idx]   # (B, T, F) float32\n",
    "            yb = ytr[idx]   # (B,) float32\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits_TB = model(xb)          # should return (T, B, 1) or (T, B)\n",
    "            logits_last = logits_TB[-1].squeeze(-1)  # (B,)\n",
    "            loss = loss_fn(logits_last, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            ep_loss += loss.item() * len(idx)\n",
    "\n",
    "        print(f\"[FWP] epoch {ep+1}/{epochs} train_loss={ep_loss/n:.6f}\")\n",
    "\n",
    "    # Eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_TB = model(Xte)\n",
    "        logits_last = logits_TB[-1].squeeze(-1)\n",
    "        probs = torch.sigmoid(logits_last).cpu().numpy()\n",
    "        preds = (probs >= 0.5).astype(int)\n",
    "\n",
    "    return preds, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a6c64fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FWP] epoch 1/2 train_loss=0.682812\n",
      "[FWP] epoch 2/2 train_loss=0.625217\n",
      "[FWP] epoch 1/2 train_loss=0.683713\n",
      "[FWP] epoch 2/2 train_loss=0.686386\n",
      "[FWP] epoch 1/2 train_loss=0.608857\n",
      "[FWP] epoch 2/2 train_loss=0.579451\n",
      "FWP summary (mean Â± std across windows):\n",
      "acc: 0.6667 Â± 0.4714\n",
      "prec: 0.3333 Â± 0.4714\n",
      "rec: 0.3333 Â± 0.4714\n",
      "f1: 0.3333 Â± 0.4714\n"
     ]
    }
   ],
   "source": [
    "FWP_EPOCHS = 2\n",
    "FWP_BATCH  = 16\n",
    "SEQ_LEN    = 20  # keep same as your LSTM seq_len if you want parity\n",
    "\n",
    "fwp_metrics = []\n",
    "fwp_accs = []\n",
    "\n",
    "for w_i, (train, test) in enumerate(windows, start=1):\n",
    "    \n",
    "    X_train_s, y_train, X_test_s, y_test = preprocess_window(train, test, feature_cols)\n",
    "\n",
    "    X_train_seq, y_train_seq = make_sequences(X_train_s, y_train, SEQ_LEN)\n",
    "    X_test_seq,  y_test_seq  = make_sequences(X_test_s,  y_test,  SEQ_LEN)\n",
    "\n",
    "    if len(X_train_seq) == 0 or len(X_test_seq) == 0:\n",
    "        continue\n",
    "\n",
    "    preds, probs = train_fwp_one_window(\n",
    "        X_train_seq, y_train_seq, X_test_seq, y_test_seq,\n",
    "        epochs=FWP_EPOCHS, batch_size=FWP_BATCH, lr=1e-2,\n",
    "        a_dim=4, latent_dim=8, q_depth=2, seed=42\n",
    "    )\n",
    "\n",
    "    m = eval_binary(y_test_seq, preds)\n",
    "    fwp_metrics.append(m)\n",
    "    fwp_accs.append(m[\"acc\"])\n",
    "\n",
    "fwp_summary = {k: (float(np.mean([m[k] for m in fwp_metrics])),\n",
    "                   float(np.std([m[k] for m in fwp_metrics])))\n",
    "               for k in [\"acc\",\"prec\",\"rec\",\"f1\"]}\n",
    "\n",
    "print(\"FWP summary (mean Â± std across windows):\")\n",
    "for k, (mu, sd) in fwp_summary.items():\n",
    "    print(f\"{k}: {mu:.4f} Â± {sd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7603f1e",
   "metadata": {},
   "source": [
    "Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa1ec6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_table(name_to_summary):\n",
    "    # name_to_summary: dict[str, dict(metric -> (mean,std))]\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "    print(\"Model\".ljust(10), *(m.ljust(20) for m in metrics))\n",
    "    for name, summ in name_to_summary.items():\n",
    "        row = [name.ljust(10)]\n",
    "        for m in metrics:\n",
    "            mu, sd = summ[m]\n",
    "            row.append(f\"{mu:.4f} Â± {sd:.4f}\".ljust(20))\n",
    "        print(*row)\n",
    "\n",
    "# name_to_summary = {\"RF\": rf_summary, \"SVM\": svm_summary, \"LSTM\": lstm_summary, \"QLSTM\": qlstm_summary, ...}\n",
    "# print_summary_table(name_to_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82d70fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_summary = {\"RF\": rf_summary, \"SVM\": svm_summary, \"LSTM\": lstm_summary, \"QLSTM\": qlstm_summary}\n",
    "#print_summary_table(name_to_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
